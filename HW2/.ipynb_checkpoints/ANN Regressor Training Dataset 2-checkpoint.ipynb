{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import backend.ann as nn \n",
    "import backend.layers as lyr\n",
    "import backend.activations as activ\n",
    "import backend.optimizers as optim\n",
    "import backend.losses as losses\n",
    "import backend.initializers as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229, 2)\n",
      "(229, 1) (229, 1)\n",
      "(92, 2)\n",
      "(92, 1) (92, 1)\n"
     ]
    }
   ],
   "source": [
    "train2 = pd.read_csv('train2.txt', delimiter='\\t', names=['x','y'])\n",
    "train2.reset_index(drop=True, inplace=True)\n",
    "train2= train2.to_numpy()\n",
    "print(train2.shape)\n",
    "train2x = train2[:,0,np.newaxis]\n",
    "train2y = train2[:,1,np.newaxis]\n",
    "print(train2x.shape, train2y.shape)\n",
    "\n",
    "test2 = pd.read_csv('test2.txt', delimiter='\\t', names=['x','y'])\n",
    "test2.reset_index(drop=True, inplace=True)\n",
    "test2 = test2.to_numpy()\n",
    "print(test2.shape)\n",
    "test2x = test2[:,0,np.newaxis]\n",
    "test2y = test2[:,1,np.newaxis]\n",
    "print(test2x.shape, test2y.shape)\n",
    "\n",
    "#normalize\n",
    "train2xNor = (train2x - np.mean(train2x))/np.std(train2x)\n",
    "train2yNor = (train2y - np.mean(train2y))/np.std(train2y)\n",
    "test2xNor = (test2x - np.mean(train2x))/np.std(train2x)\n",
    "test2yNor = (test2y - np.mean(train2y))/np.std(train2y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 0: Input Dim: 1, Number of Neurons: 8 Activation: tanh\n",
      "Layer 1: Input Dim: 8, Number of Neurons: 1 Activation: linear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = nn.ANNRegressor()\n",
    "layers = []\n",
    "HIDDEN = 8\n",
    "layers.append(lyr.Dense(train2xNor.shape[1], HIDDEN, activation=activ.Tanh(), \n",
    "                        kernel_initializer=init.RandomUniform(minval=-0.0005, maxval=0.0005),\n",
    "                         bias_initializer=init.Zeros()))\n",
    "layers.append(lyr.Dense(HIDDEN, 1, activation=activ.Linear(),\n",
    "                        kernel_initializer=init.RandomUniform(minval=-0.0005, maxval=0.0005),\n",
    "                        bias_initializer=init.Zeros()))\n",
    "\n",
    "model(layers)\n",
    "\n",
    "opt = optim.Momentum(lr=0.001, momentum=0.1)#lr=0.0004 worked for adam\n",
    "loss = losses.MSE()\n",
    "metrics = ['train_loss']\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------epoch 1----------------\n",
      "train_loss:1.0006520031345683\n",
      "----------------epoch 2----------------\n",
      "train_loss:1.0027841349038518\n",
      "----------------epoch 3----------------\n",
      "train_loss:1.001453687233057\n",
      "----------------epoch 4----------------\n",
      "train_loss:0.9994777879397955\n",
      "----------------epoch 5----------------\n",
      "train_loss:0.9995810652120435\n",
      "----------------epoch 6----------------\n",
      "train_loss:1.0013272138726264\n",
      "----------------epoch 7----------------\n",
      "train_loss:1.0023961163276605\n",
      "----------------epoch 8----------------\n",
      "train_loss:1.002315123627989\n",
      "----------------epoch 9----------------\n",
      "train_loss:1.0022918787975543\n",
      "----------------epoch 10----------------\n",
      "train_loss:1.0022643541999026\n",
      "----------------epoch 11----------------\n",
      "train_loss:1.0024738718983794\n",
      "----------------epoch 12----------------\n",
      "train_loss:1.0029412906501562\n",
      "----------------epoch 13----------------\n",
      "train_loss:1.0034245533016166\n",
      "----------------epoch 14----------------\n",
      "train_loss:1.0040915252119134\n",
      "----------------epoch 15----------------\n",
      "train_loss:1.00462488958901\n",
      "----------------epoch 16----------------\n",
      "train_loss:1.005095973057894\n",
      "----------------epoch 17----------------\n",
      "train_loss:1.0062206681529933\n",
      "----------------epoch 18----------------\n",
      "train_loss:1.0075094567697518\n",
      "----------------epoch 19----------------\n",
      "train_loss:1.0086669999634228\n",
      "----------------epoch 20----------------\n",
      "train_loss:1.0084326805735557\n",
      "----------------epoch 21----------------\n",
      "train_loss:1.00967453119212\n",
      "----------------epoch 22----------------\n",
      "train_loss:1.0109226422566073\n",
      "----------------epoch 23----------------\n",
      "train_loss:1.009164156475484\n",
      "----------------epoch 24----------------\n",
      "train_loss:1.0073965009078805\n",
      "----------------epoch 25----------------\n",
      "train_loss:1.007028133028499\n",
      "----------------epoch 26----------------\n",
      "train_loss:1.0066686322057208\n",
      "----------------epoch 27----------------\n",
      "train_loss:1.0063459284004264\n",
      "----------------epoch 28----------------\n",
      "train_loss:1.006013286536424\n",
      "----------------epoch 29----------------\n",
      "train_loss:1.0057478632741004\n",
      "----------------epoch 30----------------\n",
      "train_loss:1.007332616828384\n",
      "----------------epoch 31----------------\n",
      "train_loss:1.0074257944051066\n",
      "----------------epoch 32----------------\n",
      "train_loss:1.0074403372311564\n",
      "----------------epoch 33----------------\n",
      "train_loss:1.00740636697524\n",
      "----------------epoch 34----------------\n",
      "train_loss:1.0056201790256551\n",
      "----------------epoch 35----------------\n",
      "train_loss:1.0037623730016854\n",
      "----------------epoch 36----------------\n",
      "train_loss:1.0037698431397692\n",
      "----------------epoch 37----------------\n",
      "train_loss:1.0018881850602435\n",
      "----------------epoch 38----------------\n",
      "train_loss:1.0018151820330272\n",
      "----------------epoch 39----------------\n",
      "train_loss:1.001815608649938\n",
      "----------------epoch 40----------------\n",
      "train_loss:1.0019722631463315\n",
      "----------------epoch 41----------------\n",
      "train_loss:1.0019935068985495\n",
      "----------------epoch 42----------------\n",
      "train_loss:1.002066196078657\n",
      "----------------epoch 43----------------\n",
      "train_loss:1.0016682681761842\n",
      "----------------epoch 44----------------\n",
      "train_loss:1.001138407735049\n",
      "----------------epoch 45----------------\n",
      "train_loss:1.0012141675467416\n",
      "----------------epoch 46----------------\n",
      "train_loss:0.9960173230725397\n",
      "----------------epoch 47----------------\n",
      "train_loss:0.9944428107142369\n",
      "----------------epoch 48----------------\n",
      "train_loss:0.9957825704495886\n",
      "----------------epoch 49----------------\n",
      "train_loss:0.9950637681640214\n",
      "----------------epoch 50----------------\n",
      "train_loss:0.9976624335247735\n",
      "----------------epoch 51----------------\n",
      "train_loss:0.9950110027486853\n",
      "----------------epoch 52----------------\n",
      "train_loss:0.9947769237987106\n",
      "----------------epoch 53----------------\n",
      "train_loss:0.9636036822159425\n",
      "----------------epoch 54----------------\n",
      "train_loss:0.9517912803083869\n",
      "----------------epoch 55----------------\n",
      "train_loss:0.9287505194597929\n",
      "----------------epoch 56----------------\n",
      "train_loss:1.0008724132866056\n",
      "----------------epoch 57----------------\n",
      "train_loss:1.0375532552283833\n",
      "----------------epoch 58----------------\n",
      "train_loss:1.0541885194604879\n",
      "----------------epoch 59----------------\n",
      "train_loss:1.0462592765329546\n",
      "----------------epoch 60----------------\n",
      "train_loss:1.0387924022779393\n",
      "----------------epoch 61----------------\n",
      "train_loss:1.031885251317817\n",
      "----------------epoch 62----------------\n",
      "train_loss:1.0253033759460362\n",
      "----------------epoch 63----------------\n",
      "train_loss:1.0232511339802428\n",
      "----------------epoch 64----------------\n",
      "train_loss:1.0250090555107123\n",
      "----------------epoch 65----------------\n",
      "train_loss:1.022916253189563\n",
      "----------------epoch 66----------------\n",
      "train_loss:1.0207782716133829\n",
      "----------------epoch 67----------------\n",
      "train_loss:1.0187473199231876\n",
      "----------------epoch 68----------------\n",
      "train_loss:1.0077174952723111\n",
      "----------------epoch 69----------------\n",
      "train_loss:1.006143835461286\n",
      "----------------epoch 70----------------\n",
      "train_loss:1.000077818561694\n",
      "----------------epoch 71----------------\n",
      "train_loss:0.9940338410794197\n",
      "----------------epoch 72----------------\n",
      "train_loss:0.9834681128767256\n",
      "----------------epoch 73----------------\n",
      "train_loss:0.9570476168676945\n",
      "----------------epoch 74----------------\n",
      "train_loss:0.9545901409050669\n",
      "----------------epoch 75----------------\n",
      "train_loss:1.0477430801867813\n",
      "----------------epoch 76----------------\n",
      "train_loss:1.061528774259533\n",
      "----------------epoch 77----------------\n",
      "train_loss:1.047619258783593\n",
      "----------------epoch 78----------------\n",
      "train_loss:1.0353269684524202\n",
      "----------------epoch 79----------------\n",
      "train_loss:1.0228189589163652\n",
      "----------------epoch 80----------------\n",
      "train_loss:1.0095304222781227\n",
      "----------------epoch 81----------------\n",
      "train_loss:0.9973354396184582\n",
      "----------------epoch 82----------------\n",
      "train_loss:0.990070953620359\n",
      "----------------epoch 83----------------\n",
      "train_loss:0.9862091687647806\n",
      "----------------epoch 84----------------\n",
      "train_loss:0.981274269082712\n",
      "----------------epoch 85----------------\n",
      "train_loss:0.9441593958260823\n",
      "----------------epoch 86----------------\n",
      "train_loss:0.9444114544863086\n",
      "----------------epoch 87----------------\n",
      "train_loss:0.9610899495851882\n",
      "----------------epoch 88----------------\n",
      "train_loss:0.9498439711796556\n",
      "----------------epoch 89----------------\n",
      "train_loss:0.9672800495881699\n",
      "----------------epoch 90----------------\n",
      "train_loss:0.9782261393495654\n",
      "----------------epoch 91----------------\n",
      "train_loss:0.974761004017832\n",
      "----------------epoch 92----------------\n",
      "train_loss:0.9387827775021056\n",
      "----------------epoch 93----------------\n",
      "train_loss:0.9403852873025307\n",
      "----------------epoch 94----------------\n",
      "train_loss:1.0269260396231814\n",
      "----------------epoch 95----------------\n",
      "train_loss:1.0064505934211223\n",
      "----------------epoch 96----------------\n",
      "train_loss:1.010943559764354\n",
      "----------------epoch 97----------------\n",
      "train_loss:1.0071246934522957\n",
      "----------------epoch 98----------------\n",
      "train_loss:1.0101929799637348\n",
      "----------------epoch 99----------------\n",
      "train_loss:0.9852910222647501\n",
      "----------------epoch 100----------------\n",
      "train_loss:0.9907280201184108\n",
      "----------------epoch 101----------------\n",
      "train_loss:0.9732949595989627\n",
      "----------------epoch 102----------------\n",
      "train_loss:0.9311259673119661\n",
      "----------------epoch 103----------------\n",
      "train_loss:0.9348410878718755\n",
      "----------------epoch 104----------------\n",
      "train_loss:0.9528629629853336\n",
      "----------------epoch 105----------------\n",
      "train_loss:1.0584187471883835\n",
      "----------------epoch 106----------------\n",
      "train_loss:1.0296604900599038\n",
      "----------------epoch 107----------------\n",
      "train_loss:0.9906099032655429\n",
      "----------------epoch 108----------------\n",
      "train_loss:0.9857598908560721\n",
      "----------------epoch 109----------------\n",
      "train_loss:0.9736305601193063\n",
      "----------------epoch 110----------------\n",
      "train_loss:0.9762643046015524\n",
      "----------------epoch 111----------------\n",
      "train_loss:1.0048398039185078\n",
      "----------------epoch 112----------------\n",
      "train_loss:1.037124446178245\n",
      "----------------epoch 113----------------\n",
      "train_loss:1.0011461785598794\n",
      "----------------epoch 114----------------\n",
      "train_loss:0.993361791514184\n",
      "----------------epoch 115----------------\n",
      "train_loss:1.034607249336786\n",
      "----------------epoch 116----------------\n",
      "train_loss:1.0054986578512237\n",
      "----------------epoch 117----------------\n",
      "train_loss:1.0122624097223307\n",
      "----------------epoch 118----------------\n",
      "train_loss:1.0955286344147346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------epoch 119----------------\n",
      "train_loss:1.0434029275129748\n",
      "----------------epoch 120----------------\n",
      "train_loss:1.0466949784382635\n",
      "----------------epoch 121----------------\n",
      "train_loss:0.9769910961919053\n",
      "----------------epoch 122----------------\n",
      "train_loss:0.9380291617008946\n",
      "----------------epoch 123----------------\n",
      "train_loss:0.988390615809449\n",
      "----------------epoch 124----------------\n",
      "train_loss:1.0869160632233177\n",
      "----------------epoch 125----------------\n",
      "train_loss:1.0182884686911204\n",
      "----------------epoch 126----------------\n",
      "train_loss:1.002309228005345\n",
      "----------------epoch 127----------------\n",
      "train_loss:1.0075245618887299\n",
      "----------------epoch 128----------------\n",
      "train_loss:0.9718897565186763\n",
      "----------------epoch 129----------------\n",
      "train_loss:0.938829865146323\n",
      "----------------epoch 130----------------\n",
      "train_loss:1.0299206485803178\n",
      "----------------epoch 131----------------\n",
      "train_loss:1.0335614364784957\n",
      "----------------epoch 132----------------\n",
      "train_loss:1.0308512378857362\n",
      "----------------epoch 133----------------\n",
      "train_loss:1.026268638116174\n",
      "----------------epoch 134----------------\n",
      "train_loss:0.9752880159900965\n",
      "----------------epoch 135----------------\n",
      "train_loss:0.920394868544798\n",
      "----------------epoch 136----------------\n",
      "train_loss:0.9553513403104494\n",
      "----------------epoch 137----------------\n",
      "train_loss:1.02921063106581\n",
      "----------------epoch 138----------------\n",
      "train_loss:0.9802042110971709\n",
      "----------------epoch 139----------------\n",
      "train_loss:1.0909769706154295\n",
      "----------------epoch 140----------------\n",
      "train_loss:1.0467417534269727\n",
      "----------------epoch 141----------------\n",
      "train_loss:1.060101149013946\n",
      "----------------epoch 142----------------\n",
      "train_loss:1.0346634730298683\n",
      "----------------epoch 143----------------\n",
      "train_loss:0.986996514035908\n",
      "----------------epoch 144----------------\n",
      "train_loss:0.931034816618106\n",
      "----------------epoch 145----------------\n",
      "train_loss:1.0789624246182608\n",
      "----------------epoch 146----------------\n",
      "train_loss:1.0321888878190706\n",
      "----------------epoch 147----------------\n",
      "train_loss:1.0100439351420338\n",
      "----------------epoch 148----------------\n",
      "train_loss:0.9725530906803544\n",
      "----------------epoch 149----------------\n",
      "train_loss:0.9758554550681796\n",
      "----------------epoch 150----------------\n",
      "train_loss:1.081563070781151\n",
      "----------------epoch 151----------------\n",
      "train_loss:1.044420011655753\n",
      "----------------epoch 152----------------\n",
      "train_loss:1.0817201412019393\n",
      "----------------epoch 153----------------\n",
      "train_loss:1.0152619651392119\n",
      "----------------epoch 154----------------\n",
      "train_loss:1.002596121901927\n",
      "----------------epoch 155----------------\n",
      "train_loss:0.9643117671159648\n",
      "----------------epoch 156----------------\n",
      "train_loss:1.0519846604900247\n",
      "----------------epoch 157----------------\n",
      "train_loss:1.0602054087787078\n",
      "----------------epoch 158----------------\n",
      "train_loss:0.9973545736152636\n",
      "----------------epoch 159----------------\n",
      "train_loss:0.9647568429144414\n",
      "----------------epoch 160----------------\n",
      "train_loss:1.045056726770462\n",
      "----------------epoch 161----------------\n",
      "train_loss:0.9970208185775081\n",
      "----------------epoch 162----------------\n",
      "train_loss:1.0605354777896117\n",
      "----------------epoch 163----------------\n",
      "train_loss:1.0060845743072981\n",
      "----------------epoch 164----------------\n",
      "train_loss:0.9498310185957918\n",
      "----------------epoch 165----------------\n",
      "train_loss:1.0449116430300252\n",
      "----------------epoch 166----------------\n",
      "train_loss:1.0031279613717847\n",
      "----------------epoch 167----------------\n",
      "train_loss:1.014998798585611\n",
      "----------------epoch 168----------------\n",
      "train_loss:1.0793028633899886\n",
      "----------------epoch 169----------------\n",
      "train_loss:1.0312119437896796\n",
      "----------------epoch 170----------------\n",
      "train_loss:1.1020543776126637\n",
      "----------------epoch 171----------------\n",
      "train_loss:1.0694075267843588\n",
      "----------------epoch 172----------------\n",
      "train_loss:1.040914319985201\n",
      "----------------epoch 173----------------\n",
      "train_loss:1.0157576486202922\n",
      "----------------epoch 174----------------\n",
      "train_loss:0.9763902716288949\n",
      "----------------epoch 175----------------\n",
      "train_loss:1.0600382418150178\n",
      "----------------epoch 176----------------\n",
      "train_loss:1.0194312124133087\n",
      "----------------epoch 177----------------\n",
      "train_loss:1.063649138721027\n",
      "----------------epoch 178----------------\n",
      "train_loss:1.022474539081555\n",
      "----------------epoch 179----------------\n",
      "train_loss:1.0547040755910446\n",
      "----------------epoch 180----------------\n",
      "train_loss:1.0591073491705512\n",
      "----------------epoch 181----------------\n",
      "train_loss:1.1146361634137951\n",
      "----------------epoch 182----------------\n",
      "train_loss:1.0746811991617138\n",
      "----------------epoch 183----------------\n",
      "train_loss:1.001340787503364\n",
      "----------------epoch 184----------------\n",
      "train_loss:0.9920251383871093\n",
      "----------------epoch 185----------------\n",
      "train_loss:1.0212991204198156\n",
      "----------------epoch 186----------------\n",
      "train_loss:0.9827555068654876\n",
      "----------------epoch 187----------------\n",
      "train_loss:1.077488211716627\n",
      "----------------epoch 188----------------\n",
      "train_loss:1.0088184390845805\n",
      "----------------epoch 189----------------\n",
      "train_loss:0.9250699218756308\n",
      "----------------epoch 190----------------\n",
      "train_loss:0.9620428211951858\n",
      "----------------epoch 191----------------\n",
      "train_loss:1.018707623866529\n",
      "----------------epoch 192----------------\n",
      "train_loss:1.0067084420527301\n",
      "----------------epoch 193----------------\n",
      "train_loss:1.0904392689193343\n",
      "----------------epoch 194----------------\n",
      "train_loss:1.0526669112135403\n",
      "----------------epoch 195----------------\n",
      "train_loss:0.9777437427460214\n",
      "----------------epoch 196----------------\n",
      "train_loss:0.9389081482328707\n",
      "----------------epoch 197----------------\n",
      "train_loss:1.0116554483960936\n",
      "----------------epoch 198----------------\n",
      "train_loss:1.0513648120633547\n",
      "----------------epoch 199----------------\n",
      "train_loss:1.002425835459797\n",
      "----------------epoch 200----------------\n",
      "train_loss:0.9731655845507028\n",
      "----------------epoch 201----------------\n",
      "train_loss:1.0166676069776166\n",
      "----------------epoch 202----------------\n",
      "train_loss:0.9518947282040405\n",
      "----------------epoch 203----------------\n",
      "train_loss:1.0708607599050362\n",
      "----------------epoch 204----------------\n",
      "train_loss:1.065930131958432\n",
      "----------------epoch 205----------------\n",
      "train_loss:1.0110135553297188\n",
      "----------------epoch 206----------------\n",
      "train_loss:0.9999390317837948\n",
      "----------------epoch 207----------------\n",
      "train_loss:1.0256646809412153\n",
      "----------------epoch 208----------------\n",
      "train_loss:1.0203127428933274\n",
      "----------------epoch 209----------------\n",
      "train_loss:1.0572178289661813\n",
      "----------------epoch 210----------------\n",
      "train_loss:1.0914316741328973\n",
      "----------------epoch 211----------------\n",
      "train_loss:1.0289412455005662\n",
      "----------------epoch 212----------------\n",
      "train_loss:0.9751013700905535\n",
      "----------------epoch 213----------------\n",
      "train_loss:0.9322877985769276\n",
      "----------------epoch 214----------------\n",
      "train_loss:0.992914027520181\n",
      "----------------epoch 215----------------\n",
      "train_loss:1.064298016924782\n",
      "----------------epoch 216----------------\n",
      "train_loss:1.071114163134643\n",
      "----------------epoch 217----------------\n",
      "train_loss:0.9665146977324715\n",
      "----------------epoch 218----------------\n",
      "train_loss:0.95678726718379\n",
      "----------------epoch 219----------------\n",
      "train_loss:1.0967209195985617\n",
      "----------------epoch 220----------------\n",
      "train_loss:1.045672393623629\n",
      "----------------epoch 221----------------\n",
      "train_loss:1.0858255788559508\n",
      "----------------epoch 222----------------\n",
      "train_loss:1.0519071374776658\n",
      "----------------epoch 223----------------\n",
      "train_loss:0.9766660136896846\n",
      "----------------epoch 224----------------\n",
      "train_loss:0.9666704182380629\n",
      "----------------epoch 225----------------\n",
      "train_loss:0.988190451920465\n",
      "----------------epoch 226----------------\n",
      "train_loss:1.0743406813534122\n",
      "----------------epoch 227----------------\n",
      "train_loss:1.0096412162648227\n",
      "----------------epoch 228----------------\n",
      "train_loss:1.1308489209192496\n",
      "----------------epoch 229----------------\n",
      "train_loss:1.0721070265512727\n",
      "----------------epoch 230----------------\n",
      "train_loss:1.037179975318201\n",
      "----------------epoch 231----------------\n",
      "train_loss:0.9868573717952657\n",
      "----------------epoch 232----------------\n",
      "train_loss:0.9961722673511431\n",
      "----------------epoch 233----------------\n",
      "train_loss:0.9836502621504182\n",
      "----------------epoch 234----------------\n",
      "train_loss:0.9328004102065298\n",
      "----------------epoch 235----------------\n",
      "train_loss:1.0905918030092745\n",
      "----------------epoch 236----------------\n",
      "train_loss:0.9970214270730011\n",
      "----------------epoch 237----------------\n",
      "train_loss:0.9630212366095598\n",
      "----------------epoch 238----------------\n",
      "train_loss:0.9520264368926075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------epoch 239----------------\n",
      "train_loss:1.0565765363322597\n",
      "----------------epoch 240----------------\n",
      "train_loss:1.0083967202154582\n",
      "----------------epoch 241----------------\n",
      "train_loss:0.9429946485454621\n",
      "----------------epoch 242----------------\n",
      "train_loss:1.004528316333781\n",
      "----------------epoch 243----------------\n",
      "train_loss:1.0567610885936938\n",
      "----------------epoch 244----------------\n",
      "train_loss:0.9695846845170459\n",
      "----------------epoch 245----------------\n",
      "train_loss:0.9327822012199417\n",
      "----------------epoch 246----------------\n",
      "train_loss:1.0204132439233389\n",
      "----------------epoch 247----------------\n",
      "train_loss:1.0899171774458292\n",
      "----------------epoch 248----------------\n",
      "train_loss:1.033897148451013\n",
      "----------------epoch 249----------------\n",
      "train_loss:0.9212005587409852\n",
      "----------------epoch 250----------------\n",
      "train_loss:1.0672953461728603\n",
      "----------------epoch 251----------------\n",
      "train_loss:0.9369057672604205\n",
      "----------------epoch 252----------------\n",
      "train_loss:0.9696498657139413\n",
      "----------------epoch 253----------------\n",
      "train_loss:0.9744042504581933\n",
      "----------------epoch 254----------------\n",
      "train_loss:1.076553110445423\n",
      "----------------epoch 255----------------\n",
      "train_loss:0.9939121364626169\n",
      "----------------epoch 256----------------\n",
      "train_loss:0.9330138824197173\n",
      "----------------epoch 257----------------\n",
      "train_loss:1.088198143118803\n",
      "----------------epoch 258----------------\n",
      "train_loss:1.2456335561009126\n",
      "----------------epoch 259----------------\n",
      "train_loss:0.9981177756328031\n",
      "----------------epoch 260----------------\n",
      "train_loss:0.9324421086526247\n",
      "----------------epoch 261----------------\n",
      "train_loss:1.1641610367849828\n",
      "----------------epoch 262----------------\n",
      "train_loss:1.0042686505903151\n",
      "----------------epoch 263----------------\n",
      "train_loss:0.9298515812743102\n",
      "----------------epoch 264----------------\n",
      "train_loss:1.0035491425309278\n",
      "----------------epoch 265----------------\n",
      "train_loss:0.9660461844389726\n",
      "----------------epoch 266----------------\n",
      "train_loss:0.940166720247642\n",
      "----------------epoch 267----------------\n",
      "train_loss:0.9780092047381265\n",
      "----------------epoch 268----------------\n",
      "train_loss:1.1658256230653425\n",
      "----------------epoch 269----------------\n",
      "train_loss:1.028005762658189\n",
      "----------------epoch 270----------------\n",
      "train_loss:1.0149421592263277\n",
      "----------------epoch 271----------------\n",
      "train_loss:1.004122254783834\n",
      "----------------epoch 272----------------\n",
      "train_loss:0.9647390262972312\n",
      "----------------epoch 273----------------\n",
      "train_loss:1.209826518539966\n",
      "----------------epoch 274----------------\n",
      "train_loss:1.063611991144736\n",
      "----------------epoch 275----------------\n",
      "train_loss:0.983732442339452\n",
      "----------------epoch 276----------------\n",
      "train_loss:0.998423090793123\n",
      "----------------epoch 277----------------\n",
      "train_loss:1.125497186192939\n",
      "----------------epoch 278----------------\n",
      "train_loss:1.0458841691291543\n",
      "----------------epoch 279----------------\n",
      "train_loss:0.9948474551280222\n",
      "----------------epoch 280----------------\n",
      "train_loss:0.9405919191927581\n",
      "----------------epoch 281----------------\n",
      "train_loss:0.9759714079751003\n",
      "----------------epoch 282----------------\n",
      "train_loss:0.9918558704007201\n",
      "----------------epoch 283----------------\n",
      "train_loss:0.9306978651400675\n",
      "----------------epoch 284----------------\n",
      "train_loss:1.0248049869346947\n",
      "----------------epoch 285----------------\n",
      "train_loss:0.9012793587659863\n",
      "----------------epoch 286----------------\n",
      "train_loss:0.9756289779839878\n",
      "----------------epoch 287----------------\n",
      "train_loss:0.8987097529667857\n",
      "----------------epoch 288----------------\n",
      "train_loss:0.9151826797114642\n",
      "----------------epoch 289----------------\n",
      "train_loss:0.9508666402901567\n",
      "----------------epoch 290----------------\n",
      "train_loss:1.0396818556050123\n",
      "----------------epoch 291----------------\n",
      "train_loss:0.9348355941222115\n",
      "----------------epoch 292----------------\n",
      "train_loss:1.2465497833614245\n",
      "----------------epoch 293----------------\n",
      "train_loss:1.0633265763614508\n",
      "----------------epoch 294----------------\n",
      "train_loss:1.0642913141133041\n",
      "----------------epoch 295----------------\n",
      "train_loss:0.9223188296731984\n",
      "----------------epoch 296----------------\n",
      "train_loss:0.9442685489862597\n",
      "----------------epoch 297----------------\n",
      "train_loss:0.9636315103871866\n",
      "----------------epoch 298----------------\n",
      "train_loss:0.9919044874764787\n",
      "----------------epoch 299----------------\n",
      "train_loss:0.9110890313714051\n",
      "----------------epoch 300----------------\n",
      "train_loss:0.9998778968896576\n",
      "----------------epoch 301----------------\n",
      "train_loss:0.9618590003663559\n",
      "----------------epoch 302----------------\n",
      "train_loss:0.9308975392709767\n",
      "----------------epoch 303----------------\n",
      "train_loss:1.0021697409020454\n",
      "----------------epoch 304----------------\n",
      "train_loss:1.371802862873815\n",
      "----------------epoch 305----------------\n",
      "train_loss:0.9321285784919829\n",
      "----------------epoch 306----------------\n",
      "train_loss:1.0094094799304703\n",
      "----------------epoch 307----------------\n",
      "train_loss:1.029091501077711\n",
      "----------------epoch 308----------------\n",
      "train_loss:0.9368404647105524\n",
      "----------------epoch 309----------------\n",
      "train_loss:0.9760866133138835\n",
      "----------------epoch 310----------------\n",
      "train_loss:0.9512395102721013\n",
      "----------------epoch 311----------------\n",
      "train_loss:0.9245240542486849\n",
      "----------------epoch 312----------------\n",
      "train_loss:0.9227468265441584\n",
      "----------------epoch 313----------------\n",
      "train_loss:0.9244346931853974\n",
      "----------------epoch 314----------------\n",
      "train_loss:0.9480373488052084\n",
      "----------------epoch 315----------------\n",
      "train_loss:0.9673380884093649\n",
      "----------------epoch 316----------------\n",
      "train_loss:0.9194399278106667\n",
      "----------------epoch 317----------------\n",
      "train_loss:0.9508263387300181\n",
      "----------------epoch 318----------------\n",
      "train_loss:1.0177330038616312\n",
      "----------------epoch 319----------------\n",
      "train_loss:0.9816086767110316\n",
      "----------------epoch 320----------------\n",
      "train_loss:0.9757634329514517\n",
      "----------------epoch 321----------------\n",
      "train_loss:1.0668638420819516\n",
      "----------------epoch 322----------------\n",
      "train_loss:1.1072639020888604\n",
      "----------------epoch 323----------------\n",
      "train_loss:0.9220746475692448\n",
      "----------------epoch 324----------------\n",
      "train_loss:1.0979508573937213\n",
      "----------------epoch 325----------------\n",
      "train_loss:1.1061569399212587\n",
      "----------------epoch 326----------------\n",
      "train_loss:0.9703117322010982\n",
      "----------------epoch 327----------------\n",
      "train_loss:0.9081079269504706\n",
      "----------------epoch 328----------------\n",
      "train_loss:0.9420724007602028\n",
      "----------------epoch 329----------------\n",
      "train_loss:1.051019913758974\n",
      "----------------epoch 330----------------\n",
      "train_loss:1.0875352409632728\n",
      "----------------epoch 331----------------\n",
      "train_loss:0.9078233021699\n",
      "----------------epoch 332----------------\n",
      "train_loss:1.0291078445485875\n",
      "----------------epoch 333----------------\n",
      "train_loss:0.9823248672156268\n",
      "----------------epoch 334----------------\n",
      "train_loss:0.9505420807652933\n",
      "----------------epoch 335----------------\n",
      "train_loss:1.1186756915407532\n",
      "----------------epoch 336----------------\n",
      "train_loss:1.0026600765118077\n",
      "----------------epoch 337----------------\n",
      "train_loss:0.9077122423042839\n",
      "----------------epoch 338----------------\n",
      "train_loss:1.0809291046420348\n",
      "----------------epoch 339----------------\n",
      "train_loss:0.9747660841540615\n",
      "----------------epoch 340----------------\n",
      "train_loss:1.0706094242038313\n",
      "----------------epoch 341----------------\n",
      "train_loss:0.9730291722105259\n",
      "----------------epoch 342----------------\n",
      "train_loss:1.0368834412084889\n",
      "----------------epoch 343----------------\n",
      "train_loss:1.0870479951836673\n",
      "----------------epoch 344----------------\n",
      "train_loss:1.089417092770399\n",
      "----------------epoch 345----------------\n",
      "train_loss:1.0732858867278023\n",
      "----------------epoch 346----------------\n",
      "train_loss:1.2561681558046074\n",
      "----------------epoch 347----------------\n",
      "train_loss:1.0351981105645431\n",
      "----------------epoch 348----------------\n",
      "train_loss:0.9039928297682727\n",
      "----------------epoch 349----------------\n",
      "train_loss:1.0273778092563886\n",
      "----------------epoch 350----------------\n",
      "train_loss:1.0485067125509928\n",
      "----------------epoch 351----------------\n",
      "train_loss:0.9674040758155366\n",
      "----------------epoch 352----------------\n",
      "train_loss:0.9806226675697336\n",
      "----------------epoch 353----------------\n",
      "train_loss:1.1746785688368575\n",
      "----------------epoch 354----------------\n",
      "train_loss:1.1877751074119893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------epoch 355----------------\n",
      "train_loss:1.107933943400141\n",
      "----------------epoch 356----------------\n",
      "train_loss:1.1449618212006243\n",
      "----------------epoch 357----------------\n",
      "train_loss:1.0566542879638792\n",
      "----------------epoch 358----------------\n",
      "train_loss:0.9891952511839217\n",
      "----------------epoch 359----------------\n",
      "train_loss:1.0599886384252215\n",
      "----------------epoch 360----------------\n",
      "train_loss:0.9353591872793996\n",
      "----------------epoch 361----------------\n",
      "train_loss:0.9574860902307678\n",
      "----------------epoch 362----------------\n",
      "train_loss:1.032105490718315\n",
      "----------------epoch 363----------------\n",
      "train_loss:0.9829984433970869\n",
      "----------------epoch 364----------------\n",
      "train_loss:0.9715198846374777\n",
      "----------------epoch 365----------------\n",
      "train_loss:1.021452341551107\n",
      "----------------epoch 366----------------\n",
      "train_loss:0.9037108159931085\n",
      "----------------epoch 367----------------\n",
      "train_loss:1.03086949878195\n",
      "----------------epoch 368----------------\n",
      "train_loss:0.9338320052547596\n",
      "----------------epoch 369----------------\n",
      "train_loss:0.9237284353803984\n",
      "----------------epoch 370----------------\n",
      "train_loss:0.9073831128539026\n",
      "----------------epoch 371----------------\n",
      "train_loss:0.9730828559169975\n",
      "----------------epoch 372----------------\n",
      "train_loss:0.9698306469762363\n",
      "----------------epoch 373----------------\n",
      "train_loss:1.0018676221544314\n",
      "----------------epoch 374----------------\n",
      "train_loss:0.9390136985310665\n",
      "----------------epoch 375----------------\n",
      "train_loss:0.9995259755755479\n",
      "----------------epoch 376----------------\n",
      "train_loss:1.1058901937279006\n",
      "----------------epoch 377----------------\n",
      "train_loss:1.252099528717654\n",
      "----------------epoch 378----------------\n",
      "train_loss:1.0011139061639345\n",
      "----------------epoch 379----------------\n",
      "train_loss:0.9368114341917787\n",
      "----------------epoch 380----------------\n",
      "train_loss:1.0269256559610982\n",
      "----------------epoch 381----------------\n",
      "train_loss:0.9066029584821417\n",
      "----------------epoch 382----------------\n",
      "train_loss:0.954022163759749\n",
      "----------------epoch 383----------------\n",
      "train_loss:0.9412205795754559\n",
      "----------------epoch 384----------------\n",
      "train_loss:1.0950163370606707\n",
      "----------------epoch 385----------------\n",
      "train_loss:0.9743919336716436\n",
      "----------------epoch 386----------------\n",
      "train_loss:0.9848462449586824\n",
      "----------------epoch 387----------------\n",
      "train_loss:0.9097848993172531\n",
      "----------------epoch 388----------------\n",
      "train_loss:1.061787319003955\n",
      "----------------epoch 389----------------\n",
      "train_loss:1.0153887718914503\n",
      "----------------epoch 390----------------\n",
      "train_loss:1.0963266350409737\n",
      "----------------epoch 391----------------\n",
      "train_loss:10.98515265299568\n",
      "----------------epoch 392----------------\n",
      "train_loss:51.30257592572935\n",
      "----------------epoch 393----------------\n",
      "train_loss:60.51920520929128\n",
      "----------------epoch 394----------------\n",
      "train_loss:49.73592402776329\n",
      "----------------epoch 395----------------\n",
      "train_loss:32.71464190042806\n",
      "----------------epoch 396----------------\n",
      "train_loss:17.659703612847828\n",
      "----------------epoch 397----------------\n",
      "train_loss:8.642209159609358\n",
      "----------------epoch 398----------------\n",
      "train_loss:4.312591572627821\n",
      "----------------epoch 399----------------\n",
      "train_loss:2.0829329498161475\n",
      "----------------epoch 400----------------\n",
      "train_loss:1.3023272744483025\n",
      "----------------epoch 401----------------\n",
      "train_loss:1.0724911071746623\n",
      "----------------epoch 402----------------\n",
      "train_loss:0.970756419767559\n",
      "----------------epoch 403----------------\n",
      "train_loss:0.9288144527445493\n",
      "----------------epoch 404----------------\n",
      "train_loss:0.9070003484773386\n",
      "----------------epoch 405----------------\n",
      "train_loss:0.8761780058689804\n",
      "----------------epoch 406----------------\n",
      "train_loss:0.8658531185152419\n",
      "----------------epoch 407----------------\n",
      "train_loss:0.8706783909847533\n",
      "----------------epoch 408----------------\n",
      "train_loss:0.8589568522473275\n",
      "----------------epoch 409----------------\n",
      "train_loss:0.8714919736933001\n",
      "----------------epoch 410----------------\n",
      "train_loss:0.8686604372406517\n",
      "----------------epoch 411----------------\n",
      "train_loss:0.8572781499750045\n",
      "----------------epoch 412----------------\n",
      "train_loss:0.8688854760454255\n",
      "----------------epoch 413----------------\n",
      "train_loss:0.8630638047345437\n",
      "----------------epoch 414----------------\n",
      "train_loss:0.8590534637555358\n",
      "----------------epoch 415----------------\n",
      "train_loss:0.8656656808000358\n",
      "----------------epoch 416----------------\n",
      "train_loss:0.8714556362004375\n",
      "----------------epoch 417----------------\n",
      "train_loss:0.8751751714457239\n",
      "----------------epoch 418----------------\n",
      "train_loss:0.8669354357965814\n",
      "----------------epoch 419----------------\n",
      "train_loss:0.8647930983154419\n",
      "----------------epoch 420----------------\n",
      "train_loss:0.8763601655765795\n",
      "----------------epoch 421----------------\n",
      "train_loss:0.861371878268939\n",
      "----------------epoch 422----------------\n",
      "train_loss:0.867036138099857\n",
      "----------------epoch 423----------------\n",
      "train_loss:0.8628427801053358\n",
      "----------------epoch 424----------------\n",
      "train_loss:0.8760292291936722\n",
      "----------------epoch 425----------------\n",
      "train_loss:0.8885686862882735\n",
      "----------------epoch 426----------------\n",
      "train_loss:0.8607302004493425\n",
      "----------------epoch 427----------------\n",
      "train_loss:0.8724197820888405\n",
      "----------------epoch 428----------------\n",
      "train_loss:0.8754437288270601\n",
      "----------------epoch 429----------------\n",
      "train_loss:0.8684384101075264\n",
      "----------------epoch 430----------------\n",
      "train_loss:0.8806033014310812\n",
      "----------------epoch 431----------------\n",
      "train_loss:0.8690506665118178\n",
      "----------------epoch 432----------------\n",
      "train_loss:0.8668455913889421\n",
      "----------------epoch 433----------------\n",
      "train_loss:0.8822657871065368\n",
      "----------------epoch 434----------------\n",
      "train_loss:0.8749021378569066\n",
      "----------------epoch 435----------------\n",
      "train_loss:0.8922662238572788\n",
      "----------------epoch 436----------------\n",
      "train_loss:0.8739706072271834\n",
      "----------------epoch 437----------------\n",
      "train_loss:0.8803126160508415\n",
      "----------------epoch 438----------------\n",
      "train_loss:0.8736280119283425\n",
      "----------------epoch 439----------------\n",
      "train_loss:0.8789427822190242\n",
      "----------------epoch 440----------------\n",
      "train_loss:0.8779689500031971\n",
      "----------------epoch 441----------------\n",
      "train_loss:0.8668176567905403\n",
      "----------------epoch 442----------------\n",
      "train_loss:0.8681377180402512\n",
      "----------------epoch 443----------------\n",
      "train_loss:0.868457239662406\n",
      "----------------epoch 444----------------\n",
      "train_loss:0.8796415181966681\n",
      "----------------epoch 445----------------\n",
      "train_loss:0.8846482926539081\n",
      "----------------epoch 446----------------\n",
      "train_loss:0.8790032532662783\n",
      "----------------epoch 447----------------\n",
      "train_loss:0.8751071237615716\n",
      "----------------epoch 448----------------\n",
      "train_loss:0.8682120212144805\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1ac25259ffe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain2xNor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain2yNor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopping_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/NN_From_Scratch/backend/ann.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, epochs, batch_size, verbose, stopping_loss, shuffle)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mxbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0moutbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#* mult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NN_From_Scratch/backend/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y, pred)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(self.samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdif\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2229\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.train(train2xNor, train2yNor, epochs=10000000, batch_size=1, verbose=True, stopping_loss=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train2xNor, train2yNor)\n",
    "result_x = np.linspace(np.min(train2xNor), np.max(train2xNor), 100)[:,np.newaxis]\n",
    "result_y = model.forward(result_x)\n",
    "plt.plot(result_x, result_y, color='red')\n",
    "plt.legend(['Regressed','Ground Truth'])\n",
    "plt.title('Training Set 2')\n",
    "plt.show()\n",
    "\n",
    "#test\n",
    "plt.scatter(test2xNor, test2yNor)\n",
    "\n",
    "result_x = np.linspace(np.min(test2xNor), np.max(test2xNor), 100)[:,np.newaxis]\n",
    "result_y = model.forward(result_x)\n",
    "plt.plot(result_x, result_y, color = 'green')\n",
    "plt.legend(['Regressed','Ground Truth'])\n",
    "plt.title('Test Set 2')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist['train_loss'])\n",
    "plt.title('Loss for Training 1')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_y = model.forward(train2xNor)\n",
    "print('mean', np.mean((result_y - train2yNor)**2))\n",
    "print('std', np.std((result_y - train2yNor)**2))\n",
    "\n",
    "result_y = model.forward(test2xNor)\n",
    "print('mean', np.mean((result_y - test2yNor)**2))\n",
    "print('std', np.std((result_y - test2yNor)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenOuts = list()\n",
    "for sample in train2xNor:\n",
    "    model.forward(sample[:,np.newaxis])\n",
    "    hiddenOuts.append(model.layers[0].activation.xCache[0,:])\n",
    "hiddenOuts = np.asarray(hiddenOuts)\n",
    "hiddenOuts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hiddenOuts.shape[1]):\n",
    "    plt.plot(train2xNor, hiddenOuts[:,i])\n",
    "plt.title('Hidden Unit Values for the Range of Outputs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bitd3d6c13e347747d4b75cdea4c808ae91"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
